{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0565e31e",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:2.2rem;color:orange;'>Stock Markets Sensitivity to Social vs. News Media Sentiment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391cfd4",
   "metadata": {},
   "source": [
    "## 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d259a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers\n",
    "#pip install yfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5fba3e",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import time\n",
    "import concurrent.futures\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424bfe38",
   "metadata": {},
   "source": [
    "## 3. Download datasets | Stock Data | News Data | Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf58f7bd",
   "metadata": {},
   "source": [
    "### 3.1 Scrap Stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "tickers= ['AAPL', 'MSFT', 'TSLA', 'AMZN', 'ATVI', 'NVDA', 'FB', 'UBER', 'V', 'MA', 'AVGO', 'CSCO', 'ADBE', 'CRM', 'AMD', 'INTC', 'NFLX']\n",
    "\n",
    "for ticker in tickers:\n",
    "#Get the data for the stock each stock\n",
    "    data = yf.download(ticker,'2021-12-01','2022-03-01')\n",
    "    data['Ticker'] = ticker\n",
    "    data.to_csv('Price_'+ str(ticker) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f5813e",
   "metadata": {},
   "source": [
    "### 3.2 Scrap News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to scrape news content, from the news links\n",
    "def linkScraper(newsLinks):\n",
    "    try:\n",
    "        linkResp = requests.get(newsLinks)\n",
    "        linkSoup = bs.BeautifulSoup(linkResp.text, 'lxml')\n",
    "        \n",
    "    except:\n",
    "        print(newsLinks) \n",
    "    try:\n",
    "        url.append(linkSoup.find('meta',{\"property\":\"og:url\"}).get('content'))\n",
    "    except:\n",
    "        url.append(np.nan)\n",
    "    try:\n",
    "        title.append(linkSoup.find('h1',{\"class\":\"text__text__1FZLe text__dark-grey__3Ml43 text__medium__1kbOh text__heading_2__1K_hh heading__base__2T28j heading__heading_2__3Fcw5\"}).text)\n",
    "    except:\n",
    "        title.append(np.nan)\n",
    "    try:        \n",
    "        date.append(linkSoup.find('span',{\"class\":\"date-line__date__23Ge-\"}).text)\n",
    "    except:\n",
    "        date.append(np.nan)\n",
    "    try:\n",
    "        author.append(linkSoup.find('a',{\"class\":\"author-name__author__1gx5k\"}).text)\n",
    "    except:\n",
    "        author.append(np.nan)\n",
    "    try:\n",
    "        content.append(linkSoup.find('div',{\"class\":\"article-body__content__3VtU3 paywall-article\"}).get_text(separator=' '))\n",
    "    except:\n",
    "        content.append(np.nan)\n",
    "    \n",
    "\n",
    "#Function to scrape all news links\n",
    "def newScraper():\n",
    "    symbols = ['Amazon', 'Apple', 'Microsoft', 'Tesla', 'Blizzard', 'Nvidia', 'Facebook',\n",
    "               'Uber', 'Mastercard', 'AMD', 'Intel', 'Netflix']\n",
    "    \n",
    "    global stock, url, title, date, author, content\n",
    "    stock = []\n",
    "    url = []\n",
    "    title = []   \n",
    "    date = []\n",
    "    author = []  \n",
    "    content = [] \n",
    "    \n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    \n",
    "    for symbol in symbols: \n",
    "        searchFormat = 'https://www.reuters.com/site-search/?query={}&offset={}&sort=newest&date=past_year'  \n",
    "        newsLinks = []\n",
    "\n",
    "        for i in range(1,70):\n",
    "            try: \n",
    "                driver.get(searchFormat.format(symbol, i*10-10))\n",
    "                time.sleep(1) \n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            for attempt in range(3):\n",
    "                try: \n",
    "                    elems = driver.find_elements_by_css_selector(\".search-results__item__2oqiX [href]\")\n",
    "                    if elems != []:\n",
    "                        for elem in elems:\n",
    "                            newsLinks.append(elem.get_attribute('href'))\n",
    "                        \n",
    "                    else:\n",
    "                        print('no elements, go to next stock')\n",
    "                        break \n",
    "                    \n",
    "                except: \n",
    "                    print ('webpage error, retrying again')\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            break\n",
    "    \n",
    " \n",
    "\n",
    "        newsLinks = list(set(newsLinks))\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor: \n",
    "            executor.map(linkScraper, newsLinks)\n",
    "        stock.extend([symbol] * len(newsLinks))\n",
    "        \n",
    "        print(str(symbol) + \" stock length is : \" + str(len(stock)))\n",
    "        print(str(symbol) + \" url length is : \" + str(len(url)))\n",
    "        print(str(symbol) + \" title length is : \" + str(len(title)))\n",
    "        print(str(symbol) + \" date length is : \" + str(len(date)))\n",
    "        print(str(symbol) + \" author length is : \" + str(len(author)))\n",
    "        print(str(symbol) + \" content length is : \" + str(len(content)))\n",
    "\n",
    "    df = pd.DataFrame({'Stock': stock, 'Date':date, 'Title': title, 'Author': author, 'Content':content, 'Url':url})  \n",
    "    return (df)\n",
    "\n",
    "\n",
    "def clean():\n",
    "    #remove symbols and advertisement words\n",
    "    global df\n",
    "    df = df.dropna()\n",
    "    df.Content = [i.replace('Register now for FREE unlimited access to Reuters.com Register ', '')\n",
    "                .replace(' Our Standards:  The Thomson Reuters Trust Principles.', '') for i in df.Content]\n",
    "    df.Content = df.Content.map(lambda x: re.sub('[,.\\'\"!+?$%]', '', x))\n",
    "    \n",
    "    #change to lower case\n",
    "    df.Content = df.Content.map(lambda x: x.lower())\n",
    "\n",
    "    #df insert ticker column for stocks\n",
    "    ticker = ['AMZN', 'AAPL', 'MSFT', 'TSLA', 'ATVI', 'NVDA', 'FB', 'UBER', 'MA', 'AMD', 'INTC', 'NFLX']\n",
    "    stockDict = dict(zip(df.Stock.unique(), ticker))\n",
    "    tickerList = list(df.Stock.map(stockDict))\n",
    "    df.insert(loc = 0, column = 'Ticker', value = tickerList)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edeab6",
   "metadata": {},
   "source": [
    "### 3.3 Scrap Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21ecef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d098a5f0",
   "metadata": {},
   "source": [
    "## 4. Data Pre-processing | Stock Data | News Data | Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987393e",
   "metadata": {},
   "source": [
    "### 4.1 News Cleaning and Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04390fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('News.csv', index_col =0)\n",
    "\n",
    "df.Date = pd.to_datetime(df['Date'])\n",
    "df.Date = df.Date.dt.strftime('%d/%m/%Y')\n",
    "\n",
    "df.Content = df.Content.map(lambda x: re.sub('[-=)(*><&:]', '', x))\n",
    "\n",
    "#Tokenize every single word into a list\n",
    "def token(i):\n",
    "    for i in df.Content: \n",
    "         yield(gensim.utils.simple_preprocess(str(i), deacc=True)) \n",
    "  \n",
    "    \n",
    "#Make bigrams for each news doc\n",
    "def makeBigrams(content):  \n",
    "    bigram = gensim.models.Phrases(matrix1, min_count=5, threshold=150) # Higher threshold fewer phrases.\n",
    "    return [bigram[doc] for doc in content]\n",
    "         \n",
    "\n",
    "#Lemmatization\n",
    "def lemmatization(content, allowed_postags=['NOUN', 'PROPN', 'VERB', 'ADP']):  \n",
    "    nlp = spacy.load(\"en_core_web_sm\")  #Using spaCy library, loading English package here\n",
    "    texts_out = []\n",
    "    for i in content: #Operation for each news doc\n",
    "        doc = nlp(\" \".join(i)) #Join the words together first for lemma analysis\n",
    "        #Get the lemma for each word, if the word is noun\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags]) \n",
    "    return texts_out\n",
    "\n",
    "#Remove all the stopwords\n",
    "def stopword(content):   \n",
    "    stopWords = stopwords.words('english') \n",
    "    stopWords.extend(['reuters', 'january', 'february', 'march', 'april', 'may', 'june', \n",
    "                      'july', 'august', 'september', 'october', 'november', 'devember', \n",
    "                      'london', 'los_angele', 'summary', 'new york','bengaluru', 'america' ,'north_carolina'])\n",
    "    return [[i for i in simple_preprocess(str(doc)) \n",
    "             if i not in stopWords and len(i) >=1 ] for doc in content] \n",
    "\n",
    "\n",
    "    \n",
    "#Execute step by step\n",
    "matrix1 = list(token(df.Content))  \n",
    "matrix2 = makeBigrams(matrix1)\n",
    "matrix3 = lemmatization(matrix2)\n",
    "matrix4 = stopword(matrix3) \n",
    "\n",
    "#Create Dictionary\n",
    "matrixDict = corpora.Dictionary(matrix4) \n",
    "\n",
    "#Create Corpus\n",
    "corpus = [matrixDict.doc2bow(text) for text in matrix4] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
