{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0565e31e",
   "metadata": {},
   "source": [
    "# <h1 style='font-size:2.2rem;color:orange;'>Stock Markets Sensitivity to Social vs. News Media Sentiment</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391cfd4",
   "metadata": {},
   "source": [
    "## 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d259a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers\n",
    "#pip install yfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5fba3e",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import time\n",
    "import concurrent.futures\n",
    "import re\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "import yfinance as yf\n",
    "import os\n",
    "import json\n",
    "import os,sys\n",
    "import string\n",
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424bfe38",
   "metadata": {},
   "source": [
    "## 3. Download datasets | Stock Data | News Data | Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf58f7bd",
   "metadata": {},
   "source": [
    "### 3.1 Scrap Stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b30c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "tickers= ['AAPL', 'MSFT', 'TSLA', 'AMZN', 'ATVI', 'NVDA', 'FB', 'UBER', 'V', 'MA', 'AVGO', 'CSCO', 'ADBE', 'CRM', 'AMD', 'INTC', 'NFLX']\n",
    "\n",
    "for ticker in tickers:\n",
    "#Get the data for the stock each stock\n",
    "    data = yf.download(ticker,'2021-12-01','2022-03-01')\n",
    "    data['Ticker'] = ticker\n",
    "    data.to_csv('Price_'+ str(ticker) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f5813e",
   "metadata": {},
   "source": [
    "### 3.2 Scrap News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8b0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to scrape news content, from the news links\n",
    "def linkScraper(newsLinks):\n",
    "    try:\n",
    "        linkResp = requests.get(newsLinks)\n",
    "        linkSoup = bs.BeautifulSoup(linkResp.text, 'lxml')\n",
    "        \n",
    "    except:\n",
    "        print(newsLinks) \n",
    "    try:\n",
    "        url.append(linkSoup.find('meta',{\"property\":\"og:url\"}).get('content'))\n",
    "    except:\n",
    "        url.append(np.nan)\n",
    "    try:\n",
    "        title.append(linkSoup.find('h1',{\"class\":\"text__text__1FZLe text__dark-grey__3Ml43 text__medium__1kbOh text__heading_2__1K_hh heading__base__2T28j heading__heading_2__3Fcw5\"}).text)\n",
    "    except:\n",
    "        title.append(np.nan)\n",
    "    try:        \n",
    "        date.append(linkSoup.find('span',{\"class\":\"date-line__date__23Ge-\"}).text)\n",
    "    except:\n",
    "        date.append(np.nan)\n",
    "    try:\n",
    "        author.append(linkSoup.find('a',{\"class\":\"author-name__author__1gx5k\"}).text)\n",
    "    except:\n",
    "        author.append(np.nan)\n",
    "    try:\n",
    "        content.append(linkSoup.find('div',{\"class\":\"article-body__content__3VtU3 paywall-article\"}).get_text(separator=' '))\n",
    "    except:\n",
    "        content.append(np.nan)\n",
    "    \n",
    "\n",
    "#Function to scrape all news links\n",
    "def newScraper():\n",
    "    symbols = ['Amazon', 'Apple', 'Microsoft', 'Tesla', 'Blizzard', 'Nvidia', 'Facebook',\n",
    "               'Uber', 'Mastercard', 'AMD', 'Intel', 'Netflix']\n",
    "    \n",
    "    global stock, url, title, date, author, content\n",
    "    stock = []\n",
    "    url = []\n",
    "    title = []   \n",
    "    date = []\n",
    "    author = []  \n",
    "    content = [] \n",
    "    \n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    \n",
    "    for symbol in symbols: \n",
    "        searchFormat = 'https://www.reuters.com/site-search/?query={}&offset={}&sort=newest&date=past_year'  \n",
    "        newsLinks = []\n",
    "\n",
    "        for i in range(1,70):\n",
    "            try: \n",
    "                driver.get(searchFormat.format(symbol, i*10-10))\n",
    "                time.sleep(1) \n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            for attempt in range(3):\n",
    "                try: \n",
    "                    elems = driver.find_elements_by_css_selector(\".search-results__item__2oqiX [href]\")\n",
    "                    if elems != []:\n",
    "                        for elem in elems:\n",
    "                            newsLinks.append(elem.get_attribute('href'))\n",
    "                        \n",
    "                    else:\n",
    "                        print('no elements, go to next stock')\n",
    "                        break \n",
    "                    \n",
    "                except: \n",
    "                    print ('webpage error, retrying again')\n",
    "                    continue\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            break\n",
    "    \n",
    " \n",
    "\n",
    "        newsLinks = list(set(newsLinks))\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor: \n",
    "            executor.map(linkScraper, newsLinks)\n",
    "        stock.extend([symbol] * len(newsLinks))\n",
    "        \n",
    "        print(str(symbol) + \" stock length is : \" + str(len(stock)))\n",
    "        print(str(symbol) + \" url length is : \" + str(len(url)))\n",
    "        print(str(symbol) + \" title length is : \" + str(len(title)))\n",
    "        print(str(symbol) + \" date length is : \" + str(len(date)))\n",
    "        print(str(symbol) + \" author length is : \" + str(len(author)))\n",
    "        print(str(symbol) + \" content length is : \" + str(len(content)))\n",
    "\n",
    "    df = pd.DataFrame({'Stock': stock, 'Date':date, 'Title': title, 'Author': author, 'Content':content, 'Url':url})  \n",
    "    return (df)\n",
    "\n",
    "\n",
    "def clean():\n",
    "    #remove symbols and advertisement words\n",
    "    global df\n",
    "    df = df.dropna()\n",
    "    df.Content = [i.replace('Register now for FREE unlimited access to Reuters.com Register ', '')\n",
    "                .replace(' Our Standards:  The Thomson Reuters Trust Principles.', '') for i in df.Content]\n",
    "    df.Content = df.Content.map(lambda x: re.sub('[,.\\'\"!+?$%]', '', x))\n",
    "    \n",
    "    #change to lower case\n",
    "    df.Content = df.Content.map(lambda x: x.lower())\n",
    "\n",
    "    #df insert ticker column for stocks\n",
    "    ticker = ['AMZN', 'AAPL', 'MSFT', 'TSLA', 'ATVI', 'NVDA', 'FB', 'UBER', 'MA', 'AMD', 'INTC', 'NFLX']\n",
    "    stockDict = dict(zip(df.Stock.unique(), ticker))\n",
    "    tickerList = list(df.Stock.map(stockDict))\n",
    "    df.insert(loc = 0, column = 'Ticker', value = tickerList)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edeab6",
   "metadata": {},
   "source": [
    "### 3.3 Scrap Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "snscrape_code = \"snscrape --jsonl --max-results 1 --since 2021-12-01 twitter-search \\\"#TSLA until:2021-12-02\\\" > testing_tweets.json\"\n",
    "os.system(snscrape_code)\n",
    "\n",
    "#Please change the path of the folder after finding where your testing_tweets.json is saved\n",
    "path = '/Users/kwanw4/Downloads/'\n",
    "\n",
    "\n",
    "def processTweet(tweet):\n",
    "    # process the tweets\n",
    "\n",
    "    #Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    #Remove www.* or https?://*\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',tweet)\n",
    "    #Remove @username \n",
    "    tweet = re.sub('@[^\\s]+',' ',tweet)\n",
    "    #Remove $companytag \n",
    "    tweet = re.sub('\\$[^\\s]+',' ',tweet)\n",
    "    #Replace #word with word\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet)\n",
    "    #remove punctuation\n",
    "    tweet = tweet.replace('\\'','')\n",
    "    tweet = re.sub('[%s]' % re.escape(string.punctuation), ' ', tweet) \n",
    "    #Remove additional white spaces\n",
    "    tweet = re.sub('[\\s]+', ' ', tweet)\n",
    "    #Remove newlines\n",
    "    tweet = tweet.strip('\\n')\n",
    "    return tweet\n",
    "  #end\n",
    "\n",
    "\n",
    "def StopwordsTokenizeStem(tweet):\n",
    "    tokens = [w for w in word_tokenize(tweet) if w.isalpha()]\n",
    "# Remove stopwords\n",
    "    no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tweet = [wnl.lemmatize(t) for t in no_stops]\n",
    "    ps = PorterStemmer()\n",
    "    tweet = [ps.stem(t) for t in tweet]\n",
    "    return tweet\n",
    "  #end\n",
    "\n",
    "\n",
    "tickers = ['AAPL', 'MSFT', 'TSLA']\n",
    "for i in tickers:\n",
    "# Using OS library to call CLI commands in Python\n",
    "    snscrape_code = \"snscrape --jsonl --max-results 20 --since 2021-12-01 twitter-search \\\"#{ticker} until:2021-12-02\\\" > {ticker}_tweets.json\".format(ticker = i)\n",
    "    os.system(snscrape_code)\n",
    "\n",
    "# Reads the json generated from the CLI commands above and creates a pandas dataframe\n",
    "    i = \"TSLA\"\n",
    "    read_path = str(path) + '{ticker}_tweets.json'.format(ticker = i)\n",
    "    raw_tweets = pd.read_json(read_path, lines=True)\n",
    "    raw_tweets = raw_tweets[raw_tweets.lang == 'en']\n",
    "    raw_tweets = raw_tweets[['date','content','user','replyCount','retweetCount','likeCount','quoteCount','hashtags']]\n",
    "    \n",
    "    df = pd.concat([raw_tweets.drop(['user'], axis=1), raw_tweets['user'].apply(pd.Series)], axis=1)\n",
    "    df = df[['date','content','replyCount','retweetCount','likeCount','quoteCount','hashtags', 'verified','followersCount', 'friendsCount', 'listedCount', 'mediaCount']]\n",
    "    df['ticker'] = i\n",
    "    #Remove duplicated tweets to avoid bot tweets\n",
    "    df = df.drop_duplicates(subset=['content'], keep='first')\n",
    "\n",
    "    df['content'] = df['content'].apply(processTweet)\n",
    "    #df['content'] = df['content'].apply(StopwordsTokenizeStem)\n",
    "    \n",
    "    df.to_csv(os.path.join('tweet_{ticker}.csv'.format(ticker = i)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098a5f0",
   "metadata": {},
   "source": [
    "## 4. Data Pre-processing | Stock Data | News Data | Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987393e",
   "metadata": {},
   "source": [
    "### 4.1 News Cleaning and Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04390fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('News.csv', index_col =0)\n",
    "\n",
    "df.Date = pd.to_datetime(df['Date'])\n",
    "df.Date = df.Date.dt.strftime('%d/%m/%Y')\n",
    "\n",
    "df.Content = df.Content.map(lambda x: re.sub('[-=)(*><&:]', '', x))\n",
    "\n",
    "#Tokenize every single word into a list\n",
    "def token(i):\n",
    "    for i in df.Content: \n",
    "         yield(gensim.utils.simple_preprocess(str(i), deacc=True)) \n",
    "  \n",
    "    \n",
    "#Make bigrams for each news doc\n",
    "def makeBigrams(content):  \n",
    "    bigram = gensim.models.Phrases(matrix1, min_count=5, threshold=150) # Higher threshold fewer phrases.\n",
    "    return [bigram[doc] for doc in content]\n",
    "         \n",
    "\n",
    "#Lemmatization\n",
    "def lemmatization(content, allowed_postags=['NOUN', 'PROPN', 'VERB', 'ADP']):  \n",
    "    nlp = spacy.load(\"en_core_web_sm\")  #Using spaCy library, loading English package here\n",
    "    texts_out = []\n",
    "    for i in content: #Operation for each news doc\n",
    "        doc = nlp(\" \".join(i)) #Join the words together first for lemma analysis\n",
    "        #Get the lemma for each word, if the word is noun\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags]) \n",
    "    return texts_out\n",
    "\n",
    "#Remove all the stopwords\n",
    "def stopword(content):   \n",
    "    stopWords = stopwords.words('english') \n",
    "    stopWords.extend(['reuters', 'january', 'february', 'march', 'april', 'may', 'june', \n",
    "                      'july', 'august', 'september', 'october', 'november', 'devember', \n",
    "                      'london', 'los_angele', 'summary', 'new york','bengaluru', 'america' ,'north_carolina'])\n",
    "    return [[i for i in simple_preprocess(str(doc)) \n",
    "             if i not in stopWords and len(i) >=1 ] for doc in content] \n",
    "\n",
    "\n",
    "    \n",
    "#Execute step by step\n",
    "matrix1 = list(token(df.Content))  \n",
    "matrix2 = makeBigrams(matrix1)\n",
    "matrix3 = lemmatization(matrix2)\n",
    "matrix4 = stopword(matrix3) \n",
    "\n",
    "#Create Dictionary\n",
    "matrixDict = corpora.Dictionary(matrix4) \n",
    "\n",
    "#Create Corpus\n",
    "corpus = [matrixDict.doc2bow(text) for text in matrix4] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
